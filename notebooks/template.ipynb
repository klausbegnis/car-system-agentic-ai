{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa3c1a",
   "metadata": {},
   "source": [
    "# Car System Agentic AI - Template\n",
    "\n",
    "**Project:** Agentic AI example  \n",
    "**Author:** Klaus  \n",
    "**License:** MIT License\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the car diagnostic system using LangGraph with multiple specialized nodes for input validation, reasoning, and output processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfdbcfd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cfcfd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;91m__init__.py:46\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔑 Environment loaded from: /home/klaus/Projects/car-system-agentic-ai/.env\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;91m__init__.py:48\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔑 GOOGLE_API_KEY: ✅ Found\n"
     ]
    }
   ],
   "source": [
    "exec(open('setup.py').read())\n",
    "\n",
    "from notebooks import *\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from src.data_models.graph_state import CarSystemState\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from src.tools.car import get_car_status\n",
    "from src.tools.calculations import is_trip_possible\n",
    "from src.tools.travel import recommend_locations\n",
    "from src.tools.agent_registry import list_registered_agents, invoke_agent\n",
    "from src.utils.prompt_loader import get_prompt\n",
    "from src.utils.agent_card_loader import load_agent_cards_from_file\n",
    "from src.services.agent_registry import AgentRegistry\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6571c",
   "metadata": {},
   "source": [
    "# Load Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77d6517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93mprompt_loader.py:129\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔍 PromptLoader: Loaded prompt output_guard_rail\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93mprompt_loader.py:129\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔍 PromptLoader: Loaded prompt input_guard_rail\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93mprompt_loader.py:129\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔍 PromptLoader: Loaded prompt reasoning_node\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93mprompt_loader.py:129\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔍 PromptLoader: Loaded prompt car_central\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93mprompt_loader.py:129\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔍 PromptLoader: Loaded prompt trip_planner\n"
     ]
    }
   ],
   "source": [
    "# Load all prompts\n",
    "output_guard_rail_prompt = get_prompt(\"output_guard_rail\")\n",
    "input_guard_rail_prompt = get_prompt(\"input_guard_rail\") \n",
    "reasoning_node_prompt = get_prompt(\"reasoning_node\")\n",
    "car_central_prompt = get_prompt(\"car_central\")\n",
    "trip_planner_prompt = get_prompt(\"trip_planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe5735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and register AgentCards from JSON (pt-BR)\n",
    "cards_path = Path.cwd().parent / \"data\" / \"agent_cards.json\"\n",
    "agent_cards = load_agent_cards_from_file(cards_path)\n",
    "AgentRegistry.clear()\n",
    "\n",
    "# Extract specific cards by name\n",
    "car_card = next(c for c in agent_cards if c.name == \"AgenteDiagnosticoCarro\")\n",
    "trip_card = next(c for c in agent_cards if c.name == \"AgentePlanejadorViagem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940d3a4",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4d94d",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d489588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:52\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔗 Gemini: tools bound -> ['get_car_status']\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:58\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🤖 GeminiModel: Initialized with model gemini-2.5-flash [agent=AgenteDiagnosticoCarro]\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:52\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔗 Gemini: tools bound -> ['recommend_locations']\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:58\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🤖 GeminiModel: Initialized with model gemini-2.5-flash [agent=AgentePlanejadorViagem]\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:63\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🤖 GeminiModel: Initialized with model gemini-2.5-flash\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:52\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔗 Gemini: tools bound -> ['list_registered_agents', 'invoke_agent', 'is_trip_possible']\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:63\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🤖 GeminiModel: Initialized with model gemini-2.5-flash\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:63\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🤖 GeminiModel: Initialized with model gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Specialized agents\n",
    "\n",
    "car_agent_model = models.GeminiModel(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    prompt=car_central_prompt,\n",
    "    agent_card=car_card,\n",
    "    tools=[get_car_status],\n",
    ")\n",
    "\n",
    "trip_planner_agent_model = models.GeminiModel(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    prompt=trip_planner_prompt,\n",
    "    agent_card=trip_card,\n",
    "    tools=[recommend_locations],\n",
    ")\n",
    "\n",
    "# Register cards with their models\n",
    "AgentRegistry.register(car_card, car_agent_model)\n",
    "AgentRegistry.register(trip_card, trip_planner_agent_model)\n",
    "\n",
    "# Graph agents\n",
    "\n",
    "# Input guard rail agent\n",
    "input_guard_rail_agent = models.GeminiModel(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    prompt=input_guard_rail_prompt\n",
    ")\n",
    "\n",
    "# Reasoning agent (orchestration + quick feasibility)\n",
    "reasoning_agent = models.GeminiModel(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    prompt=reasoning_node_prompt,\n",
    "    tools=[list_registered_agents, invoke_agent, is_trip_possible],\n",
    ")\n",
    "\n",
    "# Output guard rail agent\n",
    "output_guard_rail_agent = models.GeminiModel(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    prompt=output_guard_rail_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64c756",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e301d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93minput_guard_rail.py:36\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | InputGuardRail: Initialized\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95m_node_with_tools.py:50\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | NodeWithTools: Initialized\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94moutput_guard_rail.py:41\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | OutputGuardRail: Initialized\n"
     ]
    }
   ],
   "source": [
    "# node definition\n",
    "ENTRYPOINT = START\n",
    "INPUT_GUARD_RAIL = \"input_guard_rail\"\n",
    "REASONING_NODE = \"reasoning_node\"\n",
    "OUTPUT_GUARD_RAIL = \"output_guard_rail\"\n",
    "EXIT_ZONE = \"end\"\n",
    "\n",
    "input_guard_rail = nodes.InputGuardRail(\n",
    "    routing_options={\"next_node\": REASONING_NODE, \"end\": OUTPUT_GUARD_RAIL},\n",
    "    model=input_guard_rail_agent,\n",
    ")\n",
    "\n",
    "reasoning_node = nodes.ReasoningNode(\n",
    "    routing_options={\"next_node\": OUTPUT_GUARD_RAIL, \"end\": OUTPUT_GUARD_RAIL},\n",
    "    model=reasoning_agent,\n",
    ")\n",
    "\n",
    "output_guard_rail = nodes.OutputGuardRail(\n",
    "    routing_options={\"end\": EXIT_ZONE},\n",
    "    model=output_guard_rail_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12174ec2",
   "metadata": {},
   "source": [
    "## Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213ceb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph with state_schema parameter\n",
    "workflow = StateGraph(state_schema=CarSystemState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(INPUT_GUARD_RAIL, input_guard_rail)\n",
    "workflow.add_node(REASONING_NODE, reasoning_node)\n",
    "workflow.add_node(OUTPUT_GUARD_RAIL, output_guard_rail)\n",
    "\n",
    "# Define the flow with conditional routing\n",
    "# Os nodes usam Command para determinar o próximo node baseado no estado\n",
    "workflow.add_edge(START, \"input_guard_rail\")\n",
    "#workflow.add_edge(EXIT_ZONE, lambda state: Command(goto=END))\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fd2ac",
   "metadata": {},
   "source": [
    "# Test Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b6aa706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🔄 Running synchronous version...\n",
      "🔧 Testing graph execution (synchronous)...\n",
      "Input message: Quero viajar 300 km amanhã. Com o combustível que tenho dá para ir sem reabastecer?\n",
      "------------------------------------------------------------\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93minput_guard_rail.py:44\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🛡️ InputGuardRail: Starting execution\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:36:57\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93minput_guard_rail.py:59\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 📨 Processing message: Quero viajar 300 km amanhã. Com o combustível que tenho dá para ir sem reabastecer?...\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:00\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:97\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🪙 Token usage: {'input_tokens': 385, 'output_tokens': 112, 'total_tokens': 497, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 95}}\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:00\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93minput_guard_rail.py:71\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | ✅ Validation result: is_valid=True\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:00\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;93minput_guard_rail.py:77\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🚀 Routing to next_node: reasoning_node\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:00\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mreasoning_node.py:28\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🧠 ReasoningNode: Starting execution\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:00\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mreasoning_node.py:48\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 📨 Processing message: Quero viajar 300 km amanhã. Com o combustível que tenho dá para ir sem reabastecer?...\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:02\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🪙 Token usage: {'input_tokens': 841, 'output_tokens': 144, 'total_tokens': 985, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 132}}\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:02\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;34m_chat_model.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🧰 invoke_with_tools: tool_calls=['list_registered_agents']\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:02\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;35magent_registry.py:27\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🗂️ list_registered_agents: 2 agentes\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:03\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🪙 Token usage: {'input_tokens': 925, 'output_tokens': 36, 'total_tokens': 961, 'input_token_details': {'cache_read': 700}}\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:03\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;34m_chat_model.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🧰 invoke_with_tools: tool_calls=['invoke_agent']\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:03\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;35magent_registry.py:36\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔁 invoke_agent: agent='AgenteDiagnosticoCarro', query[:120]='Qual o status do meu combustível e autonomia?'\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:03\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94magent_registry.py:98\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🧩 AgentRegistry.invoke: agent='AgenteDiagnosticoCarro', query[:120]='Qual o status do meu combustível e autonomia?'\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:04\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🪙 Token usage: {'input_tokens': 338, 'output_tokens': 63, 'total_tokens': 401, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 51}}\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:04\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;34m_chat_model.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🧰 invoke_with_tools: tool_calls=['get_car_status']\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:04\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mcar.py:24\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔧 Getting car status: gas_liters=49, current_autonomy=7\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:06\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🪙 Token usage: {'input_tokens': 387, 'output_tokens': 98, 'total_tokens': 485, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 76}}\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:06\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94magent_registry.py:127\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🧩 AgentRegistry.invoke: resposta len=75\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:06\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;35magent_registry.py:41\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔁 invoke_agent: resposta len=75\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:07\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🪙 Token usage: {'input_tokens': 997, 'output_tokens': 52, 'total_tokens': 1049, 'input_token_details': {'cache_read': 710}}\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:07\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mreasoning_node.py:71\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🚀 Routing to next_node: output_guard_rail\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:07\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94moutput_guard_rail.py:56\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🛡️ OutputGuardRail: Starting execution\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:07\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94moutput_guard_rail.py:68\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | ✅ Processing successful analysis\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:07\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94moutput_guard_rail.py:161\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🔍 Validating recommendations for safety\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:21\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95mgemini.py:106\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 🪙 Token usage: {'input_tokens': 812, 'output_tokens': 1143, 'total_tokens': 1955, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 846}}\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:21\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94moutput_guard_rail.py:204\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | ✅ Recommendations processed and validated\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:21\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;94moutput_guard_rail.py:205\u001b[37m)\u001b[0m \u001b[32mINFO\u001b[0m | 📤 Final message length: 1115 chars\n",
      "\u001b[37m[\u001b[90m2025-09-23 10:37:21\u001b[37m]\u001b[0m \u001b[37m(\u001b[1;95m_algo.py:287\u001b[37m)\u001b[0m \u001b[33mWARNING\u001b[0m | Task output_guard_rail with path ('__pregel_pull', 'output_guard_rail') wrote to unknown channel branch:to:end, ignoring it.\n",
      "------------------------------------------------------------\n",
      "✅ Synchronous graph execution successful!\n",
      "\n",
      "📊 Final State:\n",
      "------------------------------\n",
      "📝 messages: 7 messages\n",
      "   [0] HumanMessage: Quero viajar 300 km amanhã. Com o combustível que tenho dá para ir sem reabastec...\n",
      "   [1] AIMessage: ...\n",
      "   [2] ToolMessage: [{\"name\": \"AgenteDiagnosticoCarro\", \"description\": \"Agente especializado em diag...\n",
      "   [3] AIMessage: ...\n",
      "   [4] ToolMessage: Seu carro tem 49 litros de combustível e uma autonomia atual de 7 km/litro....\n",
      "   [5] AIMessage: Sim, com 49 litros de combustível e uma autonomia de 7 km/litro, seu carro tem u...\n",
      "   [6] AIMessage: Olá!\n",
      "\n",
      "Com base nos seus dados, a matemática está correta:\n",
      "*   **Capacidade do ta...\n",
      "🔹 processing_status: completed_successfully\n",
      "🔹 analysis_result: {'input': 'Quero viajar 300 km amanhã. Com o combustível que tenho dá para ir sem reabastecer?', 'analysis': 'Sim, com 49 litros de combustível e uma autonomia de 7 km/litro, seu carro tem um alcance total de 343 km. Como você quer viajar 300 km, é possível ir sem reabastecer.', 'node': 'NodeWithTools'}\n",
      "🔹 recommendations: ['Sim, com 49 litros de combustível e uma autonomia de 7 km/litro, seu carro tem um alcance total de 343 km. Como você quer viajar 300 km, é possível ir sem reabastecer.']\n",
      "\n",
      "🎉 Sync test completed! Final state has 4 populated fields.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Synchronous test (if async has issues)\n",
    "def test_graph_sync():\n",
    "    \"\"\"Synchronous version of the graph test.\"\"\"\n",
    "    \n",
    "    # Create a test state using CarSystemState\n",
    "    test_input = CarSystemState(\n",
    "        messages=[HumanMessage(content=\"Quero viajar 300 km amanhã. Com o combustível que tenho dá para ir sem reabastecer?\")],\n",
    "        user_input=None,\n",
    "        car_data=None,\n",
    "        current_node=None,\n",
    "        processing_status=None,\n",
    "        analysis_result=None,\n",
    "        recommendations=None,\n",
    "        error_message=None,\n",
    "        context=None,\n",
    "    )\n",
    "\n",
    "    config = RunnableConfig(config={\"run_name\": \"car-system-agentic-ai\"})\n",
    "\n",
    "    \n",
    "    print(\"🔧 Testing graph execution (synchronous)...\")\n",
    "    print(f\"Input message: {test_input['messages'][0].content}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Run the graph synchronously\n",
    "        result = app.invoke(input=test_input, config=config)\n",
    "        print(\"-\" * 60)\n",
    "        print(\"✅ Synchronous graph execution successful!\")\n",
    "        print(\"\\n📊 Final State:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Display the results\n",
    "        for key, value in result.items():\n",
    "            if value is not None:\n",
    "                if key == \"messages\":\n",
    "                    print(f\"📝 {key}: {len(value)} messages\")\n",
    "                    for i, msg in enumerate(value):\n",
    "                        print(f\"   [{i}] {type(msg).__name__}: {msg.content[:80]}...\")\n",
    "                elif isinstance(value, str) and len(value) > 80:\n",
    "                    print(f\"📄 {key}: {value[:80]}...\")\n",
    "                else:\n",
    "                    print(f\"🔹 {key}: {value}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during synchronous execution: {e}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔄 Running synchronous version...\")\n",
    "sync_result = test_graph_sync()\n",
    "\n",
    "if sync_result:\n",
    "    print(f\"\\n🎉 Sync test completed! Final state has {len([k for k, v in sync_result.items() if v is not None])} populated fields.\")\n",
    "else:\n",
    "    print(\"\\n❌ Sync test also failed. Check configuration.\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
